{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gridworlds\n",
    "import value_iteration\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 938.32it/s]\n",
      "100%|██████████| 172/172 [00:00<00:00, 8561.94it/s]\n",
      "Value iteration: 100%|██████████| 100/100 [00:00<00:00, 10497.57it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gridworlds.StealingGridworld(grid_size=2)\n",
    "transition_matrix, reward_vector = env.get_sparse_transition_matrix_and_reward_vector()\n",
    "optimal_qs, optimal_values = value_iteration.run_value_iteration(\n",
    "    transition_matrix, reward_vector, 100, 0.9\n",
    ")\n",
    "optimal_policy = value_iteration.get_optimal_policy_from_qs(optimal_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the StealingGridworld REPL.\n",
      "Use the following commands:\n",
      "  u: Move up\n",
      "  d: Move down\n",
      "  l: Move left\n",
      "  r: Move right\n",
      "  i: Interact with environment\n",
      "  p: Ask policy for action, if policy vector is given\n",
      "  q: Quit\n",
      "\n",
      "+---+---+\n",
      "| 0 | . |\n",
      "+---+---+\n",
      "| . | x |\n",
      "+---+---+\n",
      "Policy action: DOWN\n",
      "Reward: 0\n",
      "\n",
      "+---+---+\n",
      "| H | . |\n",
      "+---+---+\n",
      "| 0 | x |\n",
      "+---+---+\n",
      "Policy action: INTERACT\n",
      "Reward: 0\n",
      "\n",
      "+---+---+\n",
      "| H | . |\n",
      "+---+---+\n",
      "| 1 | x |\n",
      "+---+---+\n",
      "Policy action: UP\n",
      "Reward: 0\n",
      "\n",
      "+---+---+\n",
      "| 1 | . |\n",
      "+---+---+\n",
      "|   | x |\n",
      "+---+---+\n",
      "Policy action: INTERACT\n",
      "Reward: 1\n",
      "\n",
      "+---+---+\n",
      "| 0 | . |\n",
      "+---+---+\n",
      "|   | x |\n",
      "+---+---+\n",
      "Policy action: UP\n",
      "Reward: 0\n",
      "\n",
      "+---+---+\n",
      "| 0 | . |\n",
      "+---+---+\n",
      "|   | x |\n",
      "+---+---+\n",
      "Policy action: UP\n",
      "Total reward: 1\n"
     ]
    }
   ],
   "source": [
    "env.repl(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|0H | . |\n",
      "+---+---+\n",
      "| x | . |\n",
      "+---+---+\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| H |1. |\n",
      "+---+---+\n",
      "| x |   |\n",
      "+---+---+\n",
      "state: {'agent_position': array([0, 1]), 'free_pellet_locations': array([[0, 1]]), 'owned_pellet_locations': array([[1, 0]]), 'num_carried_pellets': 1}\n",
      "        UP | q_value: 0.8999999761581421\n",
      "      DOWN | q_value: 0.809999942779541\n",
      "      LEFT | q_value: 0.809999942779541\n",
      "     RIGHT | q_value: 0.7289999127388\n",
      "  INTERACT | q_value: -0.38000011444091797\n",
      "policy: UP\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "print(f\"state: {env._get_observation()}\")\n",
    "for action in range(env.action_space.n):\n",
    "    print(f\"  {env._action_to_string(action):>8} | q_value: {optimal_qs[env.get_state_index(env._get_observation()), action]}\")\n",
    "policy_action = optimal_policy[env.get_state_index(env._get_observation())]\n",
    "print(f\"policy: {env._action_to_string(policy_action)}\")\n",
    "\n",
    "_ = env.step(policy_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:\n",
      "{'agent_position': array([0, 1]), 'free_pellet_locations': array([[0, 1]]), 'owned_pellet_locations': array([[1, 0]]), 'num_carried_pellets': 1}\n",
      "\n",
      "transition matrix successor: 122\n",
      "{'agent_position': array([1, 0]), 'free_pellet_locations': array([[0, 1]]), 'owned_pellet_locations': array([], shape=(0, 2), dtype=int64), 'num_carried_pellets': 2}\n",
      "\n",
      "Environment successor: 123\n",
      "{'agent_position': array([0, 1]), 'free_pellet_locations': array([], shape=(0, 2), dtype=int64), 'owned_pellet_locations': array([[1, 0]]), 'num_carried_pellets': 2}\n"
     ]
    }
   ],
   "source": [
    "state = env._get_observation()\n",
    "state_index = env.get_state_index(state)\n",
    "succ_state = env.successor(state, 4)[0]\n",
    "succ_state_index = env.get_state_index(succ_state)\n",
    "optimal_qs[succ_state_index]\n",
    "s_a_idx = np.ravel_multi_index((state_index, 4), (len(env.states), env.action_space.n))\n",
    "transition_succ_idx = np.where(transition_matrix[s_a_idx].toarray() == 1)[1][0]\n",
    "print(f\"state:\\n{state}\\n\")\n",
    "print(f\"transition matrix successor: {transition_succ_idx}\\n{env.states[transition_succ_idx]}\\n\")\n",
    "print(f\"Environment successor: {succ_state_index}\\n{succ_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 634.65it/s]\n",
      "100%|██████████| 819/819 [00:00<00:00, 9547.93it/s]\n"
     ]
    }
   ],
   "source": [
    "tiny_env = gridworlds.StealingGridworld(grid_size=3, num_free_pellets=1, num_owned_pellets=1)\n",
    "tiny_transition, tiny_reward = tiny_env.get_sparse_transition_matrix_and_reward_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = gridworlds.StealingGridworld.LEFT\n",
    "for state in tiny_env.states:\n",
    "    state_idx = tiny_env.get_state_index(state)\n",
    "    succ_state = tiny_env.successor(state, ACTION)[0]\n",
    "    succ_state_index = tiny_env.get_state_index(succ_state)\n",
    "    s_a_idx = np.ravel_multi_index((state_idx, ACTION), (len(tiny_env.states), len(tiny_env.actions)))\n",
    "    transition_succ_idx = np.where(tiny_transition[s_a_idx].toarray() == 1)[1][0]\n",
    "    if succ_state_index != transition_succ_idx:\n",
    "        print(\"=====================================\")\n",
    "        print(f\"state: {state_idx}\\n{state}\")\n",
    "        tiny_env._register_state(state); tiny_env.render()\n",
    "        print(f\"\\ntransition matrix successor: {transition_succ_idx}\\n{tiny_env.states[transition_succ_idx]}\")\n",
    "        tiny_env._register_state(tiny_env.states[transition_succ_idx]); tiny_env.render()\n",
    "        print(f\"\\nEnvironment successor: {succ_state_index}\\n{succ_state}\")\n",
    "        tiny_env._register_state(succ_state); tiny_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_position': array([0, 1]), 'free_pellet_locations': array([[0, 1]]), 'owned_pellet_locations': array([[1, 0]]), 'num_carried_pellets': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agent_position': array([1, 0]),\n",
       " 'free_pellet_locations': array([[0, 1]]),\n",
       " 'owned_pellet_locations': array([], shape=(0, 2), dtype=int64),\n",
       " 'num_carried_pellets': 2}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_successor_from_transition_matrix(env, transition_matrix, state, action):\n",
    "    state_index = env.get_state_index(state)\n",
    "    s_a_idx = np.ravel_multi_index((state_index, action), (len(env.states), (len(env.actions))))\n",
    "    transition_succ_idx = np.where(transition_matrix[s_a_idx].toarray() == 1)[1][0]\n",
    "    return env.states[transition_succ_idx]\n",
    "\n",
    "print(state)\n",
    "get_successor_from_transition_matrix(tiny_env, tiny_transition, state, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assisting_bounded_humans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
